import os
import numpy as np
import torch as tc


'''é€‚ç”¨äºè¡¡é‡å½’ä¸€åŒ–è¯¯å·®ï¼Œç‰¹åˆ«æ˜¯åœ¨ ğ‘¦ å€¼èŒƒå›´å¯èƒ½æœ‰è¾ƒå¤§æ³¢åŠ¨çš„æƒ…å†µä¸‹ï¼Œå½’ä¸€åŒ–ç¡®ä¿äº†ç»“æœçš„é²æ£’æ€§ã€‚'''
def normalized_l2(x, y):
    return (((x - y) ** 2).mean(dim=1) / (y ** 2).mean(dim=1)).mean()

'''å¯¹è¾“å…¥å¼ é‡ x å®ç° å±‚å½’ä¸€åŒ– (Layer Normalization)ï¼Œå¹¶è¿”å›å½’ä¸€åŒ–åçš„ç»“æœåŠç›¸å…³ç»Ÿè®¡æ•°æ®ï¼ˆå‡å€¼å’Œæ ‡å‡†å·®ï¼‰'''
def layer_norm(x, eps=1e-8):
    avg = x.mean(dim=-1, keepdim=True)
    std = x.std(dim=-1, keepdim=True) + eps
    return (x - avg) / std, {"mu": avg, "std": std}


'''
ç”Ÿæˆä¸€ä¸ªä¸è¾“å…¥å¼ é‡å½¢çŠ¶ç›¸åŒçš„ æ©ç å¼ é‡ï¼Œåœ¨æ¯ä¸ªæ ·æœ¬çš„æœ€åä¸€ç»´ä¸­ï¼Œä»…ä¿ç•™æœ€å¤§ k ä¸ªå…ƒç´ çš„ä½ç½®ï¼Œå…¶ä»–ä½ç½®ä¸ºé›¶ã€‚
é€‚ç”¨äºé€‰æ‹©æ€§æ¿€æ´»ç¥ç»ç½‘ç»œçš„éƒ¨åˆ†æƒé‡æˆ–èŠ‚ç‚¹ã€‚
å¸¸ç”¨äºç¨€ç–åŒ–æ“ä½œï¼Œæˆ–åœ¨ Attention æ¨¡å—ä¸­ä¿ç•™éƒ¨åˆ†æ˜¾è‘—å€¼ã€‚
'''
def MaskTopK(x, k):
    val, idx = tc.topk(x, k=k, dim=-1) # å–topkåŠå…¶ç´¢å¼•ï¼Œæ²¿ç€-1ç»´åº¦
    return tc.zeros_like(x).scatter_(-1, idx ,1) # ç”Ÿæˆä¸€ä¸ªä¸è¾“å…¥å¼ é‡ x å½¢çŠ¶ç›¸åŒçš„äºŒå€¼æ©ç ï¼Œå…¶ä¸­Top-K æœ€å¤§å€¼å¯¹åº”çš„ä½ç½®ç½®ä¸º 1ï¼Œå…¶ä»–ä½ç½®ä¸º 0ã€‚


'''
SAE: æ— ç›‘ç£å­¦ä¹ æ¨¡å‹ï¼Œæ—¨åœ¨å­¦ä¹ è¾“å…¥æ•°æ®çš„ç¨€ç–è¡¨ç¤ºï¼ŒåŒæ—¶é‡æ„è¾“å…¥
'''
class SparseAutoencoder(tc.nn.Module):
    def __init__(self, d_inp, d_hide, device="cuda"): # è¾“å…¥ç»´åº¦ï¼Œéšè—å±‚ç»´åº¦
        super().__init__()
        self.monitoring = False
        self.do_edit = False
        self.dims = (d_inp, d_hide) # ---> 768ï¼Œ2^15
        self.lamda = 0.1 # ç”¨äºç¨€ç–æ­£åˆ™åŒ–çš„è¶…å‚æ•°
        self.usingmean = False
        self.mask = tc.nn.Parameter(tc.ones(d_hide), requires_grad=False) # (2^15) éšè—å±‚çš„æ©ç ï¼Œç”¨äºæ§åˆ¶å“ªäº›å•å…ƒæ¿€æ´»
        weight = tc.nn.init.kaiming_normal_(tc.zeros(d_inp, d_hide),
                                            mode="fan_out", nonlinearity="relu")
        self.W_enc = tc.nn.Parameter(weight, requires_grad=True) # (768,2^15)
        self.b_enc = tc.nn.Parameter(tc.zeros(d_hide)) # 2^15
        self.b_dec = tc.nn.Parameter(tc.zeros(d_inp)) # 768
        self.freq = tc.zeros(d_hide).to(device) # 2^15 è®°å½•éšè—å±‚æ¿€æ´»çš„é¢‘ç‡ï¼Œç”¨äºç¨€ç–æ€§æ§åˆ¶

        self.edit_set1 = None
        self.edit_set2 = None
        self.edit_weight1 = None
        self.edit_weight2 = None
        self.enf_set1 = None
        self.enf_value1 = None
        self.to(device)
    
    def reset_frequency(self):
        self.freq = tc.zeros_like(self.freq)

    @property
    def W_dec(self):
        return self.W_enc.T # (2^15, 768)

    def _decode(self, h):
        return h @ self.W_dec + self.b_dec # (l,2^15)@(2^15, 768)--->(l,768)+(768)

    def encode(self, x): # xä¸ºè¾“å…¥æ•°æ®ï¼Œåº”è¯¥æ˜¯(b,l,d_inp) å®é™…(l,768)
        self.aux_loss = 0.0
        x = x - self.b_dec # (l,768), (768)
        h = x @ self.W_enc + self.b_enc # (l,768) @ (768,2^15)--->(l,2^15)+(2^15)
        a = tc.relu(h) # è´Ÿå€¼å…¨0ï¼Œæ­£å€¼ä¿æŒä¸å˜--->(l,2^15)
        with tc.no_grad(): # è®°å½•æ¿€æ´»å€¼çš„é¢‘ç‡ âˆ£âˆ£ğ‘âˆ£âˆ£_0 ï¼Œè¿™æ˜¯ä¸€ç§ç¨€ç–åº¦ç»Ÿè®¡æ–¹æ³•ã€‚è¿™æ®µä»£ç å—ä¸­ä¸è®¡ç®—æ¢¯åº¦
            '''
            freq æ˜¯ä¸€ä¸ªä¸€ç»´å‘é‡ï¼Œé•¿åº¦ä¸ºéšè—å±‚ç»´åº¦ d_hideï¼Œè®°å½•äº†æ¯ä¸ªç¥ç»å…ƒçš„ç´¯ç§¯æ¿€æ´»æ¬¡æ•°
            å¯¹äºæ¯ä¸€åˆ—ï¼ˆæ€»å…± 2^15 åˆ—ï¼‰ï¼Œç»Ÿè®¡ 768 ä¸ªå…ƒç´ ä¸­éé›¶å…ƒç´ çš„æ•°é‡ã€‚
            è¿”å› (2^15,)
            '''
            self.freq += a.reshape(-1, a.shape[-1]).norm(p=0, dim=0) # (l,d_hidden), è®¡ç®— ğ¿0-èŒƒæ•°:å³ç»Ÿè®¡å¼ é‡ä¸­æ¯åˆ—çš„éé›¶å…ƒç´ ä¸ªæ•°ã€‚ç›¸å½“äºç»Ÿè®¡æ¯ä¸ªéšè—å±‚ç¥ç»å…ƒçš„éé›¶æ¿€æ´»æ¬¡æ•°ã€‚
        if self.alpha > 0:
            self.recons_h = self._decode(a) # (l,768) è§£ç å™¨å¯¹æ¿€æ´»å€¼ ğ‘ è¿›è¡Œè§£ç ï¼Œå¾—åˆ°é‡æ„çš„è¾“å…¥ recons_â„ï¼Œè¿™æ˜¯æ ‡å‡†çš„è§£ç ç»“æœ
            '''
            -self.freq: é€‰æ‹©æ¿€æ´»é¢‘ç‡æœ€ä½çš„1024ä¸ªéšè—å•å…ƒ
            h-(l,2^15), mask-(2^15), æ¯ä¸€è¡Œçš„å…ƒç´ éƒ½ä¸å‘é‡ a çš„å¯¹åº”å…ƒç´ ç›¸ä¹˜ã€‚
            '''
            aux_recons_h = self._decode(h * MaskTopK(-self.freq, 1024)) # åˆ©ç”¨æ¿€æ´»é¢‘ç‡ self.freqï¼Œç¨€ç–åŒ–éšè—å±‚æ¿€æ´» â„. å¯¹è´Ÿé¢‘ç‡åº”ç”¨ Top-K æ©ç ï¼Œé€‰å‡ºæ¿€æ´»é¢‘ç‡æœ€ä½çš„ 1024 ä¸ªç¥ç»å…ƒã€‚å°†è¿™äº›ç¥ç»å…ƒä¿ç•™ï¼Œå…¶ä½™ç¥ç»å…ƒç½®ä¸ºé›¶
            self.aux_loss = normalized_l2(aux_recons_h, x - self.recons_h) # è¡¡é‡ç¨€ç–åŒ–åé‡æ„ç»“æœä¸åŸå§‹é‡æ„è¯¯å·®ä¹‹é—´çš„å·®å¼‚ã€‚ç›®æ ‡æ˜¯è®©ç¨€ç–åŒ–åçš„è§£ç ç»“æœå°½é‡è´´è¿‘åŸå§‹è§£ç è¯¯å·®ï¼Œä»è€Œå¢å¼ºç¨€ç–è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ã€‚
        return a # å·²ç»è´Ÿå€¼ç½®0

    def decode(self, h):
        return h @ self.W_dec + self.b_dec

    def forward(self, x):
        self.recons_h = None
        self.aux_loss = 0
        h = self.encode(x)
        if self.do_edit:
            h = h * self.mask.unsqueeze(0)
        x_ = self.decode(h)
        return h, x_


    def generate(self, X):
        assert len(X.shape) == 3 # X æ˜¯ä¸€ä¸ªä¸‰ç»´å¼ é‡ï¼Œï¼ˆb,l,d_hiddenï¼‰
        self.recons_h = None
        self.aux_loss = 0
        h = self.encode(X[:, -1]) # (b,d_hiddenï¼‰) åªå¯¹æœ€åä¸€ä¸ªæ—¶åˆ»ç¼–ç ï¼šå› ä¸ºå®ƒæä¾›äº†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ˜¯å½“å‰æ—¶é—´ç‚¹çš„ä¿¡æ¯æ€»ç»“ï¼Œç¼–ç æ•´ä¸ªåºåˆ—çš„è¯ä¼šé‡å¤ç¼–ç 
        if self.do_edit:
            h = h * self.mask.unsqueeze(0)
        X = tc.cat([X[:, :-1], self.decode(h).unsqueeze(1)], dim=1)
        return X

    def compute_loss(self, inputs, lamda=0.1):
        actvs, recons = self(inputs) # è°ƒç”¨ self(inputs) è®¡ç®—éšè—å±‚æ¿€æ´»å€¼ h å’Œé‡æ„ç»“æœ ğ‘¥â€˜
        self.actvs = actvs
        self.l2 = normalized_l2(recons, inputs) # è¡¡é‡é‡æ„è¯¯å·®
        self.l1 = (actvs.abs().sum(dim=1) / inputs.norm(dim=1)).mean() # è®¡ç®—éšè—å±‚æ¿€æ´»å€¼çš„ L1 èŒƒæ•°æ­£åˆ™åŒ–ï¼Œè¡¡é‡éšè—å±‚æ¿€æ´»çš„ç¨€ç–æ€§
        self.l0 = actvs.norm(dim=-1, p=0).mean() # è®¡ç®—éšè—å±‚æ¿€æ´»å€¼çš„ L0 èŒƒæ•°ï¼Œç»Ÿè®¡éšè—å±‚ä¸­éé›¶æ¿€æ´»çš„å¹³å‡æ•°é‡
        self.ttl = self.l2 + self.lamda * self.l1  
        return self.ttl, self.l2, self.l1, self.l0






    def set_edit(self, masks, op, mag=1):
        assert op in {"turnoff", "enhance"}
        magnitude = -mag if op == "turnoff" else mag
        print("Totally %d features are %s with magnitude %s." % (len(masks), op, magnitude))
        group1 = list(masks)
        if len(group1) > 0:
            edit_set, edit_weight, edit_bias, edit_group = [], [], [], []
            if self.edit_set1 is not None:
                edit_set.append(self.edit_set1)
                edit_weight.append(self.edit_weight1)
                edit_bias.append(self.edit_bias1)
                edit_group.append(self.edit_group1)
            edit_set.append(self.W_enc[:, group1]) # getæŒ‡å®šæ€§è´¨çš„latent
            edit_weight.append(tc.zeros(len(group1)) + magnitude)
            edit_bias.append(self.b_enc[group1])
            edit_group.append(tc.tensor(group1).long())
            self.edit_set1 = tc.hstack(edit_set)
            self.edit_weight1 = tc.hstack(edit_weight)
            self.edit_bias1 = tc.hstack(edit_bias)
            self.edit_group1 = tc.hstack(edit_group)




    def enforce_actv(self, masks, magnitude=1, keep=True):
        print("Totally %d features are activated with magnitude %s." % (len(masks), magnitude))
        group1 = list(masks)
        if len(group1) > 0:
            edit_set, edit_bias = [], []
            if self.enf_set1 is not None and keep:
                edit_set.append(self.enf_set1)
                edit_bias.append(self.enf_bias1)
            edit_bias.append(self.b_enc[group1])
            edit_set.append(self.W_enc[:, group1]) # ï¼ˆ768,2^15ï¼‰-->(768,)
            self.enf_bias1 = tc.hstack(edit_bias)
            self.enf_set1 = tc.hstack(edit_set)#.mean(axis=1) # (l, len(mask))
            self.enf_weight1 = magnitude




    def edit_generate(self, x):
        print("è¦editçš„æ•°æ®ç»´åº¦ï¼š", x.shape)
        assert len(x.shape) == 3
        x = x.float()
        z = x 
        self.gen_step += 1
        if self.edit_set1 is not None:
            h1 = (x - self.b_dec) @ self.edit_set1 + self.edit_bias1 # åŸºäºæŒ‡å®šçš„æ€§è´¨çš„latentçš„è¡¨ç¤º
            h1 = tc.relu(h1) * self.edit_weight1.to(h1.device)  # å¼ºåº¦ å¯æ­£å¯è´Ÿ turnoffå°±æ˜¯-magï¼Œenhanceå°±æ˜¯mag
            z = z + h1 @ self.edit_set1.T # å¦‚æœ
        if self.enf_set1 is not None:
            if self.enf_weight1 >= 1.:
                print("å…ˆåˆ é™¤åå¢å¼º")
                h = (x - self.b_dec) @ self.enf_set1 + self.enf_bias1 # (768, l_m)
                z = z - tc.relu(h) @ self.enf_set1.T.to(h.device)
                z = z + self.enf_set1.mean(axis=1) * self.enf_weight1
            else:
                print("ç¼–è¾‘ï¼šI am here! å¹³å‡å¢å¼º")
                #z = z + (1. - self.enf_weight1) * self.enf_set1.mean(axis=1) 
                z = self.enf_weight1 * z +\
                    (1. - self.enf_weight1) * self.enf_set1.mean(axis=1) # ï¼ˆl,len(mask)ï¼‰, æ¯ä¸€è¡Œçš„å¹³å‡å€¼
        x = z
        return x.bfloat16() 




    # ä¸æ˜¯å¹³å‡å¢å¼ºï¼Œè€Œæ˜¯ä¼ å…¥çš„value
    def enforce_actv2(self, values, masks, magnitude=1, keep=True):
        print("Totally %d features are activated with magnitude %s." % (len(masks), magnitude))
        group1 = list(masks)
        if len(group1) > 0:
            edit_set, edit_bias = [], []
            if self.enf_set1 is not None and keep:
                edit_set.append(self.enf_set1)
                edit_bias.append(self.enf_bias1)
            edit_bias.append(self.b_enc[group1])
            edit_set.append(self.W_enc[:, group1])
            self.enf_bias1 = tc.hstack(edit_bias)
            self.enf_set1 = tc.hstack(edit_set)#.mean(axis=1) # (768*len(mask),)
            self.enf_weight1 = magnitude
            self.enf_value1 = tc.tensor(values).float()  # values æ˜¯ä½ ä¼ å…¥çš„å¼ºåº¦listï¼Œé•¿åº¦ä¸º a



    # åŠ æƒå¹³å‡è€Œä¸æ˜¯mean
    def edit_generate2(self, x):
        print("è¦editçš„æ•°æ®ç»´åº¦ï¼š", x.shape)
        assert len(x.shape) == 3
        x = x.float()
        z = x 
        self.gen_step += 1
        if self.edit_set1 is not None:
            h1 = (x - self.b_dec) @ self.edit_set1 + self.edit_bias1 # åŸºäºæŒ‡å®šçš„æ€§è´¨çš„latentçš„è¡¨ç¤º
            h1 = tc.relu(h1) * self.edit_weight1.to(h1.device)  # å¼ºåº¦ å¯æ­£å¯è´Ÿ turnoffå°±æ˜¯-magï¼Œenhanceå°±æ˜¯mag
            z = z + h1 @ self.edit_set1.T # å¦‚æœ
        if self.enf_set1 is not None:
            if self.enf_weight1 >= 1.:
                # print("åŸå§‹ï¼š", z,z.shape)
                h = (x - self.b_dec) @ self.enf_set1 + self.enf_bias1
                z = z - tc.relu(h) @ self.enf_set1.T.to(h.device)
                z = z + self.enf_set1.mean(axis=1) * self.enf_weight1
                # print("åé¢ï¼š", z,z.shape)
                # pirnt()
            else:
                print("ç¼–è¾‘ï¼šI am here åŠ æƒçš„å¢å¼º! å½’ä¸€åŒ–value")
                print("self.enf_set1:", self.enf_set1.shape)
                # print("z:", z, z.shape)
                # (1. - self.enf_weight1) * self.enf_set1.mean(axis=1)
                # ===== åŠ æƒå¹³å‡ä»£æ›¿ mean(axis=1) =====
                #weighted_mean = self.enf_set1 @ self.enf_value1.to(self.enf_set1.device)  # shape: (l,)
                # å½’ä¸€åŒ–+åŠ æƒå¹³å‡
                weights = self.enf_value1 / self.enf_value1.sum()
                weighted_mean = self.enf_set1 @ weights.to(self.enf_set1.device)  # shape: (l,)
                z = self.enf_weight1 * z + (1. - self.enf_weight1) * weighted_mean
                # print("weighted_mean:", weighted_mean, weighted_mean.shape, weights.shape)
                # z = z + (1. - self.enf_weight1) * weighted_mean
                # print("z2:", z,z.shape)
                # pirnt()
                
        x = z
        return x.bfloat16()
    

    # åŠ æƒï¼šz: å¢åŠ indexæ•°æ®çš„å½±å“ï¼Œ
    def edit_generate3(self, x):
        print("è¦editçš„æ•°æ®ç»´åº¦ï¼š", x.shape)
        assert len(x.shape) == 3
        x = x.float()
        z = x 
        self.gen_step += 1
        if self.edit_set1 is not None:
            h1 = (x - self.b_dec) @ self.edit_set1 + self.edit_bias1 # åŸºäºæŒ‡å®šçš„æ€§è´¨çš„latentçš„è¡¨ç¤º
            h1 = tc.relu(h1) * self.edit_weight1.to(h1.device)  # å¼ºåº¦ å¯æ­£å¯è´Ÿ turnoffå°±æ˜¯-magï¼Œenhanceå°±æ˜¯mag
            z = z + h1 @ self.edit_set1.T # å¦‚æœ
        if self.enf_set1 is not None:
            weights = self.enf_value1 / self.enf_value1.sum()
            weighted_mean = self.enf_set1 @ weights.to(self.enf_set1.device)  # shape: (l,)
            z = z*0.8 + self.enf_weight1 * weighted_mean
            # if self.enf_weight1 >= 1.:
            #     h = (x - self.b_dec) @ self.enf_set1 + self.enf_bias1
            #     z = z - tc.relu(h) @ self.enf_set1.T.to(h.device)
            #     z = z + self.enf_set1.mean(axis=1) * self.enf_weight1
            # else:
            #     print("ç¼–è¾‘ï¼šI am here åŠ æƒçš„å¢å¼º! å½’ä¸€åŒ–value")
            #     print("self.enf_set1:", self.enf_set1.shape)
            #     print("z:", z, z.shape)
            #     # ===== åŠ æƒå¹³å‡ä»£æ›¿ mean(axis=1) =====
            #     #weighted_mean = self.enf_set1 @ self.enf_value1.to(self.enf_set1.device)  # shape: (l,)
            #     # å½’ä¸€åŒ–+åŠ æƒå¹³å‡
            #     weights = self.enf_value1 / self.enf_value1.sum()
            #     weighted_mean = self.enf_set1 @ weights.to(self.enf_set1.device)  # shape: (l,)
            #     # z = self.enf_weight1 * z + (1. - self.enf_weight1) * weighted_mean
            #     print("weighted_mean:", weighted_mean, weighted_mean.shape, weights.shape)
            #     z = z + (1. - self.enf_weight1) * weighted_mean
            #     print("z2:", z,z.shape)
            #     pirnt()
                
        x = z
        return x.bfloat16()
    
    


    '''é’ˆå¯¹logitscore'''
    def enforce_actv_logitscore(self, latent_i, magnitude=1, keep=True):
        print("Totally %d features are activated with magnitude %s." % (1, magnitude))
        enhance_vec = self.W_enc[:, latent_i]  # (768,), å•ä¸ªå¢å¼ºçš„latent
        self.enf_set1 = enhance_vec
        self.enf_weight1 = magnitude

    def edit_generate_logitscore(self, x): # xæ˜¯è¾“å…¥çš„hidden state ï¼ˆb,l,768ï¼‰
        print("è¦editçš„æ•°æ®ç»´åº¦ï¼š", x.shape)
        assert len(x.shape) == 3
        x = x.float()
        z = x 
        enhance_vec_i = self.enf_set1.unsqueeze(0).expand(x.shape[1], -1).unsqueeze(0) # data_x(b,l,c), (1,768)-->(l,768)
        z_aug = self.enf_weight1 * z + (1 - self.enf_weight1) * enhance_vec_i
        print(enhance_vec_i.shape, z.shape, self.enf_set1.shape)
        
        return z_aug.bfloat16()




    @classmethod
    def from_disk(cls, fpath, device="cuda"):
        print("Loading SAE from %s." % fpath)
        states = tc.load(fpath)
        model = cls(**states["config"], device="cpu") # æ ¹æ®ä¿å­˜çš„é…ç½®åˆå§‹åŒ–æ¨¡å‹å¯¹è±¡ï¼Œæš‚æ—¶è®¾ç½®ä¸ºcpuè®¾å¤‡
        model.load_state_dict(states['weight'], strict=True) # å°†ä¿å­˜çš„æƒé‡åŠ è½½åˆ°æ¨¡å‹ä¸­ã€‚Trueç¡®ä¿æ¨¡å‹çš„å‚æ•°ç»“æ„ä¸åŠ è½½çš„æƒé‡å®Œå…¨åŒ¹é…
        return model.to(device)

    def dump_disk(self, fpath):
        os.makedirs(os.path.split(fpath)[0], exist_ok=True)
        tc.save({"weight": self.state_dict(), 
                 "config": {"d_inp": self.dims[0], 
                            "d_hide": self.dims[1],}
                            },
                 fpath)
        print("SAE is dumped at %s." % fpath)




class TopKSAE(SparseAutoencoder):
    def __init__(self, d_inp, d_hide, topK=20, device="cuda"):
        super().__init__(d_inp, d_hide, device)
        self.topk = topK
        print("æ£€æŸ¥topkï¼š", self.topk)
        self.lamda = 0
        self.MaskTopK = True
        self.disabled = False
        self.logging = True
        self.epsilon = 1e-6
        self.recons_h = None
        self.freq = tc.zeros(d_hide).to(device)
    
    def reset_frequency(self):
        self.freq = tc.zeros_like(self.freq)

    def _encode(self, x):
        return (x - self.b_dec) @ self.W_enc + self.b_enc

    def _decode(self, h):
        return h @ self.W_dec + self.b_dec



    # è®¡ç®—æ¿€æ´»çŠ¶æ€
    def caculate_act(self, x):  # è¾“å…¥ shape: (b, 2^15)
        print(f"Input Shape: {x.shape}")  # æ‰“å°è¾“å…¥ x çš„å½¢çŠ¶
        
        # è®¡ç®—æ¯è¡Œï¼ˆæ ·æœ¬ï¼‰çš„ç»Ÿè®¡ä¿¡æ¯
        feature_min = x.min(dim=1).values  # æ¯è¡Œæœ€å°å€¼ (shape: [b])
        feature_max = x.max(dim=1).values  # æ¯è¡Œæœ€å¤§å€¼ (shape: [b])
        feature_mean = x.mean(dim=1)  # æ¯è¡Œå‡å€¼ (shape: [b])
        feature_std = x.std(dim=1, unbiased=False)  # æ¯è¡Œæ ‡å‡†å·® (shape: [b])

        # è®¡ç®—æ¯è¡Œï¼ˆæ ·æœ¬ï¼‰ä¸­å¤§äº 0 çš„æ•°æ®ä¸ªæ•°
        positive_count = (x > 0).sum(dim=1)  # ç»Ÿè®¡æ¯è¡Œä¸­å¤§äº 0 çš„å…ƒç´ ä¸ªæ•° (shape: [b])

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("Feature Min:", feature_min)
        print("Feature Max:", feature_max)
        print("Feature Mean:", feature_mean)
        print("Feature Std Dev:", feature_std)
        print("Positive Count:", positive_count)

    # å‡è®¾ input_embedding æ˜¯ (batch_size, seq_length, hidden_dim) å½¢çŠ¶çš„è¯å‘é‡
    def check_embedding_stats(self, embedding):
        min_val = embedding.min().item()
        max_val = embedding.max().item()
        mean_val = embedding.mean().item()
        std_val = embedding.std().item()

        print(f"Embedding Stats:")
        print(f"  Min: {min_val:.4f}")
        print(f"  Max: {max_val:.4f}")
        print(f"  Mean: {mean_val:.4f}")
        print(f"  Std: {std_val:.4f}")

        # å½’ä¸€åŒ–å»ºè®®
        if abs(mean_val) > 5 or std_val > 10 or min_val < -100 or max_val > 100:
            print("âš ï¸ å»ºè®®å½’ä¸€åŒ–ï¼å¯èƒ½ä¼šå½±å“ Transformer è®­ç»ƒç¨³å®šæ€§ã€‚")
        else:
            print("âœ… ä¸éœ€è¦å½’ä¸€åŒ–ï¼Œæ•°æ®åˆ†å¸ƒæ­£å¸¸ã€‚")



    def encode(self, x):
        h = self._encode(x) # h--(l,2^15)
        # print("-----------åŸå§‹hidden state 768--------------")
        # print(x)
        # self.caculate_act(x)
        # print("-----------æ‰€æœ‰æ¿€æ´»å€¼ï¼ˆ2^15ï¼‰æ•°å€¼--------------")
        # print(h)
        # self.caculate_act(h) 

        mask = MaskTopK(h, self.topk) # (l,2^15)æ²¿ç€2^15æ‰¾æœ€å¤§ï¼Œæ¯ä¸€è¡Œæ‰¾2^15ä¸­çš„topkã€‚é€‰å–æ¯ä¸ªæ ·æœ¬ä¸­æœ€å¤§çš„ topk ä¸ªæ¿€æ´»å€¼ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªæ©ç çŸ©é˜µï¼ˆ0 æˆ– 1ï¼‰
        with tc.no_grad():
            # ç»Ÿè®¡æ©ç ä¸­æ¯ä¸ªç¥ç»å…ƒçš„æ¿€æ´»é¢‘ç‡ï¼Œå­˜å‚¨åœ¨ self.freq ä¸­
            self.freq += mask.reshape(-1, mask.shape[-1]).sum(axis=0).to(self.freq.device) # saeçš„2^15ä¸­çš„æ¯ä¸ªfeatureæ¿€æ´»äº†å¤šå°‘è¾“å…¥å‘é‡ ä¸€ä¸ªsae featureåœ¨lä¸­çš„æ¿€æ´»çŠ¶å†µ
        if self.alpha > 0: 
            self.recons_h = self._decode(h * mask) # ç”¨ç¨€ç–åŒ–åçš„æ¿€æ´»å€¼è§£ç ï¼Œå¾—åˆ°é‡æ„ç»“æœ recons_h
            aux_recons_h = self._decode(h * MaskTopK(-self.freq, 1024)) # é€šè¿‡æ¿€æ´»é¢‘ç‡æœ€ä½çš„ 1024 ä¸ªç¥ç»å…ƒè¿›è¡Œè¾…åŠ©é‡æ„
            self.aux_loss = normalized_l2(aux_recons_h, x - self.recons_h)  # è®¡ç®—ç¨€ç–åŒ–åçš„é‡æ„è¯¯å·®ï¼Œä½œä¸ºè¾…åŠ©æŸå¤±
        h = h * mask
        return tc.relu(h)
    

    def encode2(self, x):
        h = self._encode(x) # h--(l,2^15)
        return tc.relu(h)
    
    def decode(self, h):
        if self.recons_h is None:
            self.recons_h = self._decode(h)
        return self.recons_h
    
    # def forward(self, x):
    #     self.recons_h = None
    #     self.aux_loss = 0
    #     h = self.encode(x)
    #     if self.do_edit:
    #         h = h * self.mask.unsqueeze(0)
    #     x_ = self.decode(h)
    #     return h, x_


    def compute_review_act(self, inputs, lamda=0.1): # è¾“å…¥çš„å°±æ˜¯hidde state
        if len(inputs.shape) == 3:
            inputs = inputs.squeeze(0) # å¦‚æœè¾“å…¥æ˜¯ 3 ç»´ï¼Œå»æ‰ç¬¬ 0 ç»´ï¼ˆå½“ç¬¬0ç»´æ˜¯å¤§å°æ˜¯1ï¼‰
        
        latent_review = self.encode2(inputs.float()) # (l c)
        '''å–æœ€åä¸€ä¸ªç»´åº¦topkçš„æ¿€æ´»çš„ç»´åº¦'''
        #latent_review_actindex = tc.topk(latent_review[-1], 15).indices.tolist() # (l,c)æœ€åä¸€å±‚lä¸­cçš„é‚£äº›ä½ç½®
        #self.latent_review_actindex = latent_review_actindex

        '''
        1. å½’ä¸€åŒ–/ä¸å½’ä¸€åŒ–
        2. æ¯åˆ—ç»Ÿä¸€æ±‚å‡å€¼ï¼Œåªå¯¹å…¶ä¸­é0çš„ä½ç½®æ±‚å‡å€¼
        '''
        '''è€ƒè™‘æ‰€æœ‰ç»´åº¦çš„æ¿€æ´»'''
        # 1. æ‰¾å‡ºè¢«æ¿€æ´»çš„ç»´åº¦
        activated_mask = (latent_review != 0).any(dim=0) # ä¿ç•™dim1 è¿”å›true falseåˆ—è¡¨ï¼ˆc,ï¼‰
        activated_means = latent_review[:, activated_mask != 0].mean(axis=0) # å‡å€¼é’ˆå¯¹lè€Œä¸æ˜¯lä¸­æœ‰æ•°æ®çš„-å¯èƒ½æ˜¯å°äºl
        dim_strength = dict((a,b.item()) for (a,b) in zip(tc.nonzero(activated_mask, as_tuple=True)[0].tolist(), activated_means))
    
        # 3. æ’åºå¹¶è¿”å›ç»´åº¦ç´¢å¼•åˆ—è¡¨ï¼ˆæŒ‰æ¿€æ´»å¼ºåº¦ä»å¤§åˆ°å°ï¼‰,åªè¿”å›äº†indexï¼Œæ²¡æœ‰è¿”å›æ¿€æ´»çš„å¼ºåº¦ï¼Œæ•°å€¼
        sorted_dims = sorted(dim_strength.keys(), key=lambda d: dim_strength[d], reverse=True)
        self.latent_review_actindex = dim_strength
        return self.latent_review_actindex
        

        '''åªå–æœ€åä¸€å±‚'''
        # # åªå–æœ€åä¸€å±‚
        # last_layer = latent_review[-1]  # shape: (c,)

        # # æ‰¾å‡ºéé›¶æ¿€æ´»çš„ç»´åº¦
        # activated_mask = last_layer != 0
        # activated_dims = tc.nonzero(activated_mask, as_tuple=True)[0]

        # # ç»Ÿè®¡æ¯ä¸ªç»´åº¦çš„æ¿€æ´»å¼ºåº¦ï¼ˆå°±æ˜¯è¯¥ç»´åº¦çš„å€¼ï¼‰
        # dim_strength = {dim.item(): last_layer[dim].item() for dim in activated_dims} # å­—å…¸

        # # æŒ‰æ¿€æ´»å¼ºåº¦æ’åº
        # sorted_dims = sorted(dim_strength.keys(), key=lambda d: dim_strength[d], reverse=True) # æ ¹æ®å­—å…¸çš„valueæ’åºindexï¼Œindexçš„list

        # self.latent_review_actindex = dim_strength
        # return self.latent_review_actindex
    
    '''æ‰¾sequentialæ•°æ®çš„æ¿€æ´»æƒ…å†µ'''
    def compute_review_act_sequential(self, inputs, lamda=0.1): # è¾“å…¥çš„å°±æ˜¯hidde state
        if len(inputs.shape) == 3:
            inputs = inputs.squeeze(0) # å¦‚æœè¾“å…¥æ˜¯ 3 ç»´ï¼Œå»æ‰ç¬¬ 0 ç»´ï¼ˆå½“ç¬¬0ç»´æ˜¯å¤§å°æ˜¯1ï¼‰
        
        latent_review = self.encode2(inputs.float()) # (l c)
        '''å–æœ€åä¸€ä¸ªç»´åº¦topkçš„æ¿€æ´»çš„ç»´åº¦'''
        #latent_review_actindex = tc.topk(latent_review[-1], 15).indices.tolist() # (l,c)æœ€åä¸€å±‚lä¸­cçš„é‚£äº›ä½ç½®
        #self.latent_review_actindex = latent_review_actindex

        '''
        1. å½’ä¸€åŒ–/ä¸å½’ä¸€åŒ–
        2. æ¯åˆ—ç»Ÿä¸€æ±‚å‡å€¼ï¼Œåªå¯¹å…¶ä¸­é0çš„ä½ç½®æ±‚å‡å€¼
        '''
        '''è€ƒè™‘æ‰€æœ‰ç»´åº¦çš„æ¿€æ´»'''
        # 1. æ‰¾å‡ºè¢«æ¿€æ´»çš„ç»´åº¦
        activated_mask = (latent_review != 0).any(dim=0) # ä¿ç•™dim1 è¿”å›true falseåˆ—è¡¨ï¼ˆc,ï¼‰
        activated_means = latent_review[:, activated_mask != 0].mean(axis=0) # å‡å€¼é’ˆå¯¹lè€Œä¸æ˜¯lä¸­æœ‰æ•°æ®çš„-å¯èƒ½æ˜¯å°äºl
        dim_strength = dict((a,b.item()) for (a,b) in zip(tc.nonzero(activated_mask, as_tuple=True)[0].tolist(), activated_means))
    
        # 3. æ’åºå¹¶è¿”å›ç»´åº¦ç´¢å¼•åˆ—è¡¨ï¼ˆæŒ‰æ¿€æ´»å¼ºåº¦ä»å¤§åˆ°å°ï¼‰,åªè¿”å›äº†indexï¼Œæ²¡æœ‰è¿”å›æ¿€æ´»çš„å¼ºåº¦ï¼Œæ•°å€¼
        sorted_dims = sorted(dim_strength.items(), key=lambda item: item[1], reverse=True)
        self.latent_review_actindex = sorted_dims
        return self.latent_review_actindex


    '''æ‰¾sequentialæ•°æ®çš„æ¿€æ´»æƒ…å†µ'''
    def compute_review_act_sequential_last(self, inputs, lamda=0.1): # è¾“å…¥çš„å°±æ˜¯hidde state
        if len(inputs.shape) == 3:
            inputs = inputs.squeeze(0) # å¦‚æœè¾“å…¥æ˜¯ 3 ç»´ï¼Œå»æ‰ç¬¬ 0 ç»´ï¼ˆå½“ç¬¬0ç»´æ˜¯å¤§å°æ˜¯1ï¼‰
        
        latent_review = self.encode2(inputs.float()) # (l c)
        '''å–æœ€åä¸€ä¸ªç»´åº¦topkçš„æ¿€æ´»çš„ç»´åº¦'''
        #latent_review_actindex = tc.topk(latent_review[-1], 15).indices.tolist() # (l,c)æœ€åä¸€å±‚lä¸­cçš„é‚£äº›ä½ç½®
        #self.latent_review_actindex = latent_review_actindex
        '''åªå–æœ€åä¸€å±‚'''
        last_layer = latent_review[-1]  # shape: (c,)

        # æ‰¾å‡ºéé›¶æ¿€æ´»çš„ç»´åº¦
        activated_mask = last_layer != 0
        activated_dims = tc.nonzero(activated_mask, as_tuple=True)[0]

        # ç»Ÿè®¡æ¯ä¸ªç»´åº¦çš„æ¿€æ´»å¼ºåº¦ï¼ˆå°±æ˜¯è¯¥ç»´åº¦çš„å€¼ï¼‰
        dim_strength = {dim.item(): last_layer[dim].item() for dim in activated_dims} # å­—å…¸
        sorted_dims_with_values = sorted(dim_strength.items(), key=lambda d: d[1], reverse=True)

        # æŒ‰æ¿€æ´»å¼ºåº¦æ’åº
        #sorted_dims = sorted(dim_strength.keys(), key=lambda d: dim_strength[d], reverse=True) # æ ¹æ®å­—å…¸çš„valueæ’åºindexï¼Œindexçš„list

        self.latent_review_actindex = sorted_dims_with_values
        return self.latent_review_actindex










    def compute_loss(self, inputs, lamda=0.1): # è¾“å…¥çš„å°±æ˜¯hidde state
        actvs, recons = self(inputs) # actvs: encoderçš„è¾“å‡º-(l,2*15)
        self.actvs = actvs
        self.l2 = normalized_l2(recons, inputs)
        self.l1 = (actvs.abs().sum(dim=1) / inputs.norm(dim=1)).mean() # actvs(l,2^15), input(l,768)-->(l,)/(l,)--->mean
        self.l0 = actvs.norm(dim=-1, p=0).mean() # é0ä¸ªæ•°--(l,)
        self.ttl = self.l2 + self.alpha * self.aux_loss
        return self.ttl, self.l2, self.l1, self.l0
    
    def compute_loss_ab(self, inputs, a,b, lamda=0.1):
        actvs, recons = self(inputs) # actvs: encoderçš„è¾“å‡º-(l,2*15)
        self.actvs = actvs
        #print("abé•¿åº¦ï¼š", len(inputs),a,b, len(inputs[a:b])) # æœ‰æ—¶å€™len(inputs[a:b])=0
        self.l2 = normalized_l2(recons[a:b], inputs[a:b])
        #print("ceshi:", self.l2) #len(inputs[a:b])==0---ã€‹nan
        self.l1 = (actvs.abs().sum(dim=1) / inputs.norm(dim=1)).mean() # actvs(l,2^15), input(l,768)-->(l,)/(l,)--->mean
        self.l0 = actvs.norm(dim=-1, p=0).mean() # é0ä¸ªæ•°--(l,)
        self.ttl = self.l2 + self.alpha * self.aux_loss
        return self.ttl, self.l2, self.l1, self.l0
    
    def compute_loss2(self, inputs, lamda=0.1):
        print("spançš„è®¡ç®—actvs")
        inputs = inputs.squeeze()
        actvs, recons = self(inputs)
        self.l2 = normalized_l2(recons, inputs)
        self.actvs = actvs
        print("abc:", self.l2, actvs.shape, inputs.shape)
        return self.actvs
    
    
    def generate_encoder(self, inputs, lamda=0.1):
        print("è®¡ç®—actvsï¼Œåˆ¤æ–­å“ªå‡ ä¸ªç»´åº¦æ¿€æ´»äº†ï¼") # (b,l,c)
        assert len(inputs.shape) == 3
        #actvs = self.encode(inputs) # (b,l,2^15)
        print("----------------åŸå§‹æ•°æ®è¾“å…¥çš„å‡å€¼æ–¹å·® ï¼ˆblcï¼‰----------------")
        self.check_embedding_stats(inputs)

        self.actvs_eos = self.encode(inputs[:, -1].float()) # ä¸€èˆ¬æœ€åä¸€ä¸ªæ˜¯eos-1ï¼ˆb,cï¼‰ï¼Œå–è¿™å¥è¯çš„è¯­ä¹‰token
        # ç»Ÿè®¡æ¯è¡Œéé›¶å…ƒç´ çš„æ•°é‡
        self.non_zero_counts = tc.count_nonzero(self.actvs_eos, dim=1)
        print("16æ¡æ•°æ®ï¼Œæ¯æ¡æ•°æ®çš„æ¿€æ´»æƒ…å†µï¼š", self.non_zero_counts)
        self.non_zero_indices = tc.nonzero(self.actvs_eos, as_tuple=False)
        self.result = [[] for _ in range(self.actvs_eos.shape[0])]
        for row_idx, col_idx in self.non_zero_indices:
            self.result[row_idx.item()].append(col_idx.item())

        # # å‡è®¾ hidden_states æ˜¯ (batch_size, l, c)
        # eos_mask = (input_ids == tokenizer.eos_token_id)  # ç”Ÿæˆ bool mask
        # eos_hidden_state = actvs[:,-1]        # å–å‡º <eos> å¯¹åº”çš„å‘é‡ (c,)
        # eos_hidden_state = eos_hidden_state.unsqueeze(0)  # å˜ä¸º (1, c)
        
        
        return self.actvs_eos, self.non_zero_counts, self.non_zero_indices, self.result
    

    '''ä¸ºäº†sequentialâ€”review'''
    def generate_encoder2(self, inputs, lamda=0.1):
        print("è®¡ç®—actvsï¼Œåˆ¤æ–­å“ªå‡ ä¸ªç»´åº¦æ¿€æ´»äº†ï¼") # (b,l,c)
        assert len(inputs.shape) == 3 # (1,l,c)
        latent_review = self.encode2(inputs.squeeze(0).float()) # (l c)
        latent_review_actindex = tc.topk(latent_review[-1], 15).indices.tolist() 
        
        return latent_review_actindex
    




    # ç”¨é‡æ„çš„æ•°æ®
    def generate_encoder_recon(self, inputs, lamda=0.1):
        print("è®¡ç®—actvsï¼Œåˆ¤æ–­å“ªå‡ ä¸ªç»´åº¦æ¿€æ´»äº†ï¼") # (b,l,c)
        assert len(inputs.shape) == 3
        #actvs = self.encode(inputs) # (b,l,2^15)
        #print("inputs dtype: floatè¿˜æ˜¯bfloat16ï¼š", inputs[:, -1].dtype)  # æŸ¥çœ‹æ•°æ®ç±»å‹ torch.float32
        self.actvs_eos = self.encode(inputs[:, -1].float()) # ä¸€èˆ¬æœ€åä¸€ä¸ªæ˜¯eos-1ï¼ˆb,cï¼‰
        # ç»Ÿè®¡æ¯è¡Œéé›¶å…ƒç´ çš„æ•°é‡
        self.non_zero_counts = tc.count_nonzero(self.actvs_eos, dim=1)
        print("16æ¡æ•°æ®ï¼Œæ¯æ¡æ•°æ®çš„æ¿€æ´»æƒ…å†µï¼š", self.non_zero_counts)
        self.non_zero_indices = tc.nonzero(self.actvs_eos, as_tuple=False)
        self.result = [[] for _ in range(self.actvs_eos.shape[0])]
        for row_idx, col_idx in self.non_zero_indices:
            self.result[row_idx.item()].append(col_idx.item())

        if self.do_edit:
            h = self.actvs_eos * self.mask.unsqueeze(0)
        X = tc.cat([inputs[:, :-1], self.decode(self.actvs_eos).unsqueeze(1)], dim=1)
        # print("åŸå§‹æ•°æ®ï¼š", inputs,inputs.shape)
        # print("é‡æ„æ•°æ®ï¼š", X, X.shape)
        # print("è§£ç æ•°æ®ï¼š", self.decode(self.actvs_eos), self.decode(self.actvs_eos).shape)
        # if tc.equal(X, self.decode(self.actvs_eos)):
        #     print("å…¨ç­‰ï¼")
        
        return X, self.actvs_eos, self.non_zero_counts, self.non_zero_indices, self.result


    
    '''ä¸ºäº†è®¡ç®—è°æ¿€æ´»'''
    def generate(self, X):
        actvs, recons = self(X)
        print("æ¿€æ´»å€¼actvsçš„ç»´åº¦æ˜¯ï¼š", actvs.shape) # æ¿€æ´»å€¼actvsçš„ç»´åº¦æ˜¯ï¼š torch.Size([16, 1, 65536]) ï¼ˆb,l,cï¼‰

        assert len(X.shape) == 3 # X æ˜¯ä¸€ä¸ªä¸‰ç»´å¼ é‡ï¼Œï¼ˆb,l,d_hiddenï¼‰
        self.recons_h = None
        self.aux_loss = 0
        h = self.encode(X[:, -1]) # (b,d_hiddenï¼‰) åªå¯¹æœ€åä¸€ä¸ªæ—¶åˆ»ç¼–ç ï¼šå› ä¸ºå®ƒæä¾›äº†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ˜¯å½“å‰æ—¶é—´ç‚¹çš„ä¿¡æ¯æ€»ç»“ï¼Œç¼–ç æ•´ä¸ªåºåˆ—çš„è¯ä¼šé‡å¤ç¼–ç 
        
        if self.do_edit:
            h = h * self.mask.unsqueeze(0)
        X = tc.cat([X[:, :-1], self.decode(h).unsqueeze(1)], dim=1)
        return X
    
    def dump_disk(self, fpath):
        os.makedirs(os.path.split(fpath)[0], exist_ok=True)
        tc.save({"weight": self.state_dict(), 
                 "config": {"d_inp": self.dims[0], 
                            "d_hide": self.dims[1], 
                            "topK": self.topk}
                            },
                 fpath)
        print("SAE is dumped at %s." % fpath)


# "ours": DisSAE, 
SAEs = {"sae": SparseAutoencoder, "ae": SparseAutoencoder, "topk": TopKSAE, "topk2": TopKSAE}

def load_pretrained(fpath, device="cuda"):
    return SAEs["topk"].from_disk(fpath, device)
    # assert os.path.exists(fpath)
    # name = os.path.split(fpath)[-1].rsplit(".pth")[0]
    # cls = name.split("_l", 1)[0].lower()
    # layer = int(name.split("_l", 1)[1].split("_", 1)[0])
    # return name, layer, SAEs[cls].from_disk(fpath, device)


    
    # assert os.path.exists(fpath), f"æ–‡ä»¶è·¯å¾„ {fpath} ä¸å­˜åœ¨ï¼"
    
    # # æå–æ–‡ä»¶åå¹¶å»æ‰æ‰©å±•å
    # name = os.path.split(fpath)[-1].rsplit(".pth")[0]
    
    # # è§£ææ–‡ä»¶åï¼Œæå–å…³é”®ä¿¡æ¯
    # parts = name.split("_")
    # model_name = parts[0]  # æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ "TopK5"
    # layer = int(parts[1][1:])  # æå– "_l7" ä¸­çš„æ•°å­— 7
    # hidden_dim = int(parts[2][1:-1])  # æå– "_h131k" ä¸­çš„ 131000
    # epoch = int(parts[3][5:])  # æå– "epoch5" ä¸­çš„ 5
    
    # # åŠ è½½æ¨¡å‹ (è¿™é‡Œå‡è®¾ P5 æ¨¡å‹åœ¨ `SAEs` å­—å…¸ä¸­æ³¨å†Œ)
    # assert model_name.lower() in SAEs, f"æœªçŸ¥æ¨¡å‹ç±»åˆ«ï¼š{model_name.lower()}"
    # model_instance = SAEs[model_name.lower()].from_disk(fpath, device)
    
    # return name, layer, hidden_dim, epoch, model_instance
